Large language models are conditional next-token generators trained to minimize cross-entropy over a huge text corpus. Given a context window of previous tokens, the model outputs a probability distribution over the next token and samples or selects from that distribution. What looks like understanding is, at base, the ability to reproduce locally coherent patterns in text by exploiting statistical regularities in the training data. With enough data and sufficient effective context length, a purely statistical model can appear intelligent, even though it has no grounded semantics or experience of meaning.
